import os
import csv

from utilities import common, differential, parsing
from wremnants.datasets.datagroups import Datagroups

analysis_label = Datagroups.analysisLabel(os.path.basename(__file__))
parser, initargs = parsing.common_parser(analysis_label)

import math

import hist
import numpy as np
import ROOT

import narf
from wremnants import (
    helicity_utils,
    muon_calibration,
    muon_efficiencies_binned,
    muon_efficiencies_smooth,
    muon_prefiring,
    muon_selections,
    pileup,
    syst_tools,
    theory_corrections,
    theory_tools,
    theoryAgnostic_tools,
    unfolding_tools,
    vertex,
)
from wremnants.datasets.dataset_tools import getDatasets
from wremnants.histmaker_tools import (
    aggregate_groups,
    make_quantile_helper,
    scale_to_data,
    write_analysis_output,
)
from wums import logging

parser.add_argument(
    "--csVarsHist", action="store_true", help="Add CS variables to dilepton hist"
)
parser.add_argument("--axes", type=str, nargs="*", default=["mll", "ptll"], help="")
parser.add_argument(
    "--finePtBinning", action="store_true", help="Use fine binning for ptll"
)
parser.add_argument(
    "--useTheoryAgnosticBinning",
    action="store_true",
    help="Use theory agnostic binning (coarser) to produce the results",
)
parser.add_argument(
    "--useDileptonTriggerSelection",
    action="store_true",
    help="Use dilepton trigger selection (default uses the Wlike one, with one triggering muon and odd/even event selection to define its charge, staying agnostic to the other)",
)
parser.add_argument(
    "--noAuxiliaryHistograms",
    action="store_true",
    help="Remove auxiliary histograms to save memory (removed by default with --unfolding or --theoryAgnostic)",
)
parser.add_argument(
    "--muonIsolation",
    type=int,
    nargs=2,
    default=[1, 1],
    choices=[-1, 0, 1],
    help="Apply isolation cut to triggering and not-triggering muon (in this order): -1/1 for failing/passing isolation, 0 for skipping it. If using --useDileptonTriggerSelection, then the sorting is based on the muon charge as -/+",
)
parser.add_argument(
    "--flipEventNumberSplitting",
    action="store_true",
    help="Flip even with odd event numbers to consider the positive or negative muon as the W-like muon",
)
parser.add_argument(
    "--useTnpMuonVarForSF",
    action="store_true",
    help="To read efficiency scale factors, use the same muon variables as used to measure them with tag-and-probe (by default the final corrected ones are used)",
)
parser.add_argument(
    "--makeCSQuantileHists",
    action="store_true",
    help="Make hists with fine binned CS variables for producing quantiles",
)
parser.add_argument(
    "--splitSampleInN",
    type=int,
    default=-1,
    help="Split the sample in N parts, useful for debugging and testing",
)
parser.add_argument(
    "--randomSeedForSplit",
    type=int,
    default=12345,
    help="Random seed for splitting the sample in N parts",
)
parser.add_argument(
    "--jackknifeN",
    type=int,
    default=0,
    help="Number of jackknife samples to use, if > 0, then the sample is split in 2*jackknifeN parts",
)
parser.add_argument(
    "--jackknifeEfficiency",
    type=float,
    default=0.5,
    help="Jackknife efficiency, used to define the size of the sample",
)
parser.add_argument(
    "--randomSeedForJackknife",
    type=int,
    default=12345,
    help="Random seed for jackknifing procedure",
)
parser.add_argument(
    "--saveEventCsv",
    action="store_true",
    help="Save per-event CSV after selections (one row per event)",
)
parser.add_argument(
    "--eventCsvCols",
    type=str,
    nargs="*",
    default=["run", "luminosityBlock", "event", "mll", "yll", "ptll", "nominal_weight"],
    help="Columns to save in the event CSV",
)
parser.add_argument(
    "--eventCsvOutDir",
    type=str,
    default="",
    help="Output directory for event CSV files (defaults to --outfolder or CWD)",
)
parser.add_argument(
    "--eventCsvMaxEvents",
    type=int,
    default=-1,
    help="Maximum number of events to write per dataset (<=0 means all)",
)
parser.add_argument(
    "--eventCsvPrefix",
    type=str,
    default="events",
    help="Prefix for event CSV filenames",
)
parser = parsing.set_parser_default(
    # If user does no provide "aggregateGroups", the parser will use the list
    parser, "aggregateGroups", ["Diboson", "Top", "Wtaunu", "Wmunu"]
)
# Same here. Set defualt value if "excludeProcs" not provided by user
parser = parsing.set_parser_default(parser, "excludeProcs", ["QCD"])
# Same here. Set defualt values if "pt" not provided by user and default depends on the "analysis_label"
parser = parsing.set_parser_default(
    parser, "pt", common.get_default_ptbins(analysis_label)
)

# Parse the arguments (defaults used for the arguments above if not provided by user).
# Use parse_known_args here because additional arguments are added later
# (e.g. --ptllMin/--ptllMax). parse_known_args() will not raise on those
# unknown options at this stage and they will be parsed in the final
# `parser.parse_args()` call below.
args, _ = parser.parse_known_args()

logger = logging.setup_logger(__file__, args.verbose, args.noColorLogger)



# Choose analysis type
thisAnalysis = (
    ROOT.wrem.AnalysisType.Dilepton
    # if flag is passed on the command line (see above), use dilepton analysis type: sorting is based on the muon charge as -/+
    if args.useDileptonTriggerSelection
    # otherwise use Wlike (default)
    else ROOT.wrem.AnalysisType.Wlike 
)
# Get the isolation branch name based on the isolation definition
isoBranch = muon_selections.getIsoBranch(args.isolationDefinition)
# Copy the "--era" command-line argument into a variable "era" (which year/period of data to use)
era = args.era

# Build datasets
datasets = getDatasets(
    # Number of files to use per dataset
    maxFiles=args.maxFiles,
    # Filter processes to include (e.g. only W or Z samples)
    filt=args.filterProcs,
    # Filter processes to exclude
    excl=args.excludeProcs,
    # Select NanoAOD version
    nanoVersion="v9",
    # ROOT folder where the data/MC files are located
    base_path=args.dataPath,
    # if "msht20an3lo" is not in hte list of PDF sets
    extended="msht20an3lo" not in args.pdfs,
    # Take one MC file every N file
    oneMCfileEveryN=args.oneMCfileEveryN,
    # Choose the correct era (year/period of data taking)
    era=era,
)

# dilepton invariant mass cuts
mass_min, mass_max = common.get_default_mz_window()

ptll_min, ptll_max = common.get_default_ptllcut()

parser.add_argument(
    "--ptllMin",
    dest="ptll_min",
    type=float,
    default=ptll_min,
    help="Minimum dilepton pT to select Z candidates",
)
parser.add_argument(
    "--ptllMax",
    dest="ptll_max",
    type=float,
    default=ptll_max,
    help="Maximum dilepton pT to select Z candidates",
)

args = parser.parse_args()

ptll_min = args.ptll_min
ptll_max = args.ptll_max

print(f"\n\n\nptllmin = {ptll_min}, ptllmax = {ptll_max}\n")
print(f"args.ptllmin = {args.ptll_min}, args.ptllmax = {args.ptll_max}\n\n\n")

ewMassBins = theory_tools.make_ew_binning(mass=91.1535, width=2.4932, initialStep=0.010)

# Choose dilepton pt binning
if args.useTheoryAgnosticBinning:
    # Include the underflow/overflow bins in pT, absYll and wlike binning
    theoryAgnostic_axes, _ = differential.get_theoryAgnostic_axes(
        ptV_flow=True, absYV_flow=True, wlike=True
    )
    axis_ptV_thag = theoryAgnostic_axes[0]
    dilepton_ptV_binning = axis_ptV_thag.edges
else:
    dilepton_ptV_binning = common.ptZ_binning if not args.finePtBinning else range(200)

if "yll" in args.axes:
    # use 10 quantiles in case "yll" is used as nominal axis
    edges_yll = common.yll_10quantiles_binning
    edges_absYll = edges_yll[len(edges_yll) // 2 :]
    axis_yll = hist.axis.Variable(edges_yll, name="yll")
    axis_absYll = hist.axis.Variable(edges_absYll, name="absYll", underflow=False)
else:
    axis_yll = hist.axis.Regular(20, -2.5, 2.5, name="yll")
    axis_absYll = hist.axis.Regular(10, 0.0, 2.5, name="absYll", underflow=False)

# available axes for dilepton validation plots
all_axes = {
    "mll": hist.axis.Variable(
        [
            60,
            70,
            75,
            78,
            80,
            82,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            100,
            102,
            105,
            110,
            120,
        ],
        name="mll",
    ),
    "xmaxll": hist.axis.Regular(500, 0, 1, name="xmaxll"),
    "xminll": hist.axis.Regular(500, 0, 1, name="xminll"),
    # "xminmax_ll": hist.axis.Variable([[500, 0, 1], [500, 0, 1]], name="xminmax_ll"),
    "yll": axis_yll,
    "absYll": axis_absYll,
    "ptll": hist.axis.Variable(dilepton_ptV_binning, name="ptll", underflow=False),
    "etaPlus": hist.axis.Variable([-2.4, -1.2, -0.3, 0.3, 1.2, 2.4], name="etaPlus"),
    "etaMinus": hist.axis.Variable([-2.4, -1.2, -0.3, 0.3, 1.2, 2.4], name="etaMinus"),
    "etaRegionSign": hist.axis.Regular(
        3, 0, 3, name="etaRegionSign", underflow=False, overflow=False
    ),
    "etaRegionRange": hist.axis.Regular(
        3, 0, 3, name="etaRegionRange", underflow=False, overflow=False
    ),
    "absEtaPlus": hist.axis.Regular(8, 0, 2.4, name="absEtaPlus"),
    "absEtaMinus": hist.axis.Regular(8, 0, 2.4, name="absEtaMinus"),
    "etaAbsEta": hist.axis.Variable(
        [
            -2.4,
            -2.0,
            -1.6,
            -1.4,
            -1.2,
            -1.0,
            -0.6,
            0.0,
            0.6,
            1.0,
            1.2,
            1.4,
            1.6,
            2.0,
            2.4,
        ],
        name="etaAbsEta",
    ),
    "etaSum": hist.axis.Regular(12, -4.8, 4.8, name="etaSum"),
    "etaDiff": hist.axis.Variable(
        [-4.8, -1.0, -0.6, -0.2, 0.2, 0.6, 1.0, 4.8], name="etaDiff"
    ),
    "ptPlus": hist.axis.Regular(int(args.pt[0]), args.pt[1], args.pt[2], name="ptPlus"),
    "ptMinus": hist.axis.Regular(
        int(args.pt[0]), args.pt[1], args.pt[2], name="ptMinus"
    ),
    "cosThetaStarll": hist.axis.Regular(
        200 if args.makeCSQuantileHists else 20,
        -1.0,
        1.0,
        name="cosThetaStarll",
        underflow=False,
        overflow=False,
    ),
    "phiStarll": hist.axis.Regular(
        200 if args.makeCSQuantileHists else 20,
        -math.pi,
        math.pi,
        circular=True,
        name="phiStarll",
    ),
    # "charge": hist.axis.Regular(2, -2., 2., underflow=False, overflow=False, name = "charge") # categorical axes in python bindings always have an overflow bin, so use a regular
    "massVgen": hist.axis.Variable(ewMassBins, name="massVgen"),
    "ewMll": hist.axis.Variable(ewMassBins, name="ewMll"),
    "ewMlly": hist.axis.Variable(ewMassBins, name="ewMlly"),
    "ewLogDeltaM": hist.axis.Regular(100, -10, 4, name="ewLogDeltaM"),
    "trigMuons_abseta0": hist.axis.Regular(
        3, 0.0, 2.4, name="trigMuons_abseta0", underflow=False
    ),
    "nonTrigMuons_eta0": hist.axis.Regular(
        int(args.eta[0]), args.eta[1], args.eta[2], name="nonTrigMuons_eta0"
    ),
    "nonTrigMuons_pt0": hist.axis.Regular(
        int(args.pt[0]), args.pt[1], args.pt[2], name="nonTrigMuons_pt0"
    ),
    "nonTrigMuons_charge0": hist.axis.Regular(
        2, -2.0, 2.0, underflow=False, overflow=False, name="nonTrigMuons_charge0"
    ),
    "ptll_resolution": hist.axis.Regular(1000, -1, 1, name="ptll_resolution"),
}

auxiliary_gen_axes = [
    "massVgen",  # preFSR variables
    "ewMll",
    "ewMlly",
    "ewLogDeltaM",  # ew variables
]

for a in args.axes:
    if a not in all_axes.keys():
        logger.error(
            f" {a} is not a known axes! Supported axes choices are {list(all_axes.keys())}"
        )

nominal_cols = args.axes

if args.csVarsHist:
    # in case CS variables are added to the main histogram, use optimized binning
    # CS variables will be binned in nxn quantiles; quantiles are computed in each bin of args.axes as provided by the quantile_file
    n_quantiles = 8
    all_axes["cosThetaStarll_quantile"] = hist.axis.Regular(
        n_quantiles,
        0,
        1,
        name="cosThetaStarll_quantile",
        underflow=False,
        overflow=False,
    )
    all_axes["phiStarll_quantile"] = hist.axis.Regular(
        n_quantiles,
        0,
        1,
        name="phiStarll_quantile",
        underflow=False,
        overflow=False,
    )

    quantile_file = f"{common.data_dir}/angularCoefficients/mz_dilepton_scetlib_dyturbo_CT18Z_N3p0LL_N2LO_Corr_maxFiles_m1_csQuantiles.hdf5"
    quantile_helper_csVars = make_quantile_helper(
        quantile_file,
        ["cosThetaStarll", "phiStarll"],
        ["ptll", "absYll"],
        name="nominal_csQuantiles",
        processes=["Zmumu_2016PostVFP"],
        n_quantiles=[n_quantiles],
    )

    nominal_cols += ["cosThetaStarll_quantile", "phiStarll_quantile"]

nominal_axes = [all_axes[a] for a in nominal_cols]

if args.unfolding:
    add_helicity_axis = "helicitySig" in args.unfoldingAxes

    if args.unfoldingInclusive:
        cutsmap = {"fiducial": "masswindow"}
    else:
        cutsmap = {
            "pt_min": args.pt[1],
            "pt_max": args.pt[2],
            "abseta_max": args.eta[2],
            "mass_min": mass_min,
            "mass_max": mass_max,
        }

    unfolder_z = unfolding_tools.UnfolderZ(
        reco_axes_edges={a: all_axes[a].edges for a in args.axes},
        unfolding_axes_names=args.unfoldingAxes,
        unfolding_levels=args.unfoldingLevels,
        poi_as_noi=args.poiAsNoi,
        fitresult=args.fitresult,
        cutsmap=cutsmap,
    )

    if not args.poiAsNoi:
        datasets = unfolding_tools.add_out_of_acceptance(datasets, group="Zmumu")

# define helpers
muon_prefiring_helper, muon_prefiring_helper_stat, muon_prefiring_helper_syst = (
    muon_prefiring.make_muon_prefiring_helpers(era=era)
)
procs = [
    p
    for p, grp in (("W", common.wprocs), ("Z", common.zprocs))
    if any(d.name in grp for d in datasets)
]
theory_helpers_procs = theory_corrections.make_theory_helpers(
    args.pdfs, args.theoryCorr, procs=procs
)

# extra axes which can be used to label tensor_axes
if args.binnedScaleFactors:
    logger.info("Using binned scale factors and uncertainties")
    # might never use it really anymore, but let's warn the user that this is obsolete
    logger.warning(
        "Only SF with no uT dependence are implemented, and the treatment for trigger is like Wlike"
    )
    # add usePseudoSmoothing=True for tests with Asimov
    muon_efficiency_helper, muon_efficiency_helper_syst, muon_efficiency_helper_stat = (
        muon_efficiencies_binned.make_muon_efficiency_helpers_binned(
            filename=args.sfFile, era=era, max_pt=args.pt[2], is_w_like=True
        )
    )
else:
    logger.info("Using smoothed scale factors and uncertainties")
    muon_efficiency_helper, muon_efficiency_helper_syst, muon_efficiency_helper_stat = (
        muon_efficiencies_smooth.make_muon_efficiency_helpers_smooth(
            filename=args.sfFile,
            era=era,
            max_pt=args.pt[2],
            what_analysis=thisAnalysis,
            isoEfficiencySmoothing=args.isoEfficiencySmoothing,
            smooth3D=args.smooth3dsf,
            isoDefinition=args.isolationDefinition,
        )
    )
logger.info(f"SF file: {args.sfFile}")

muon_efficiency_helper_syst_altBkg = {}
for es in common.muonEfficiency_altBkgSyst_effSteps:
    altSFfile = args.sfFile.replace(".root", "_altBkg.root")
    logger.info(f"Additional SF file for alternate syst with {es}: {altSFfile}")
    muon_efficiency_helper_syst_altBkg[es] = (
        muon_efficiencies_smooth.make_muon_efficiency_helpers_smooth_altSyst(
            filename=altSFfile,
            era=era,
            what_analysis=thisAnalysis,
            max_pt=args.pt[2],
            effStep=es,
        )
    )

pileup_helper = pileup.make_pileup_helper(era=era)
vertex_helper = vertex.make_vertex_helper(era=era)

calib_filepaths = common.calib_filepaths
closure_filepaths = common.closure_filepaths
diff_weights_helper = (
    ROOT.wrem.SplinesDifferentialWeightsHelper(calib_filepaths["tflite_file"])
    if (args.muonScaleVariation == "smearingWeightsSplines" or args.validationHists)
    else None
)
(
    mc_jpsi_crctn_helper,
    data_jpsi_crctn_helper,
    mc_jpsi_crctn_unc_helper,
    data_jpsi_crctn_unc_helper,
) = muon_calibration.make_jpsi_crctn_helpers(
    args, calib_filepaths, make_uncertainty_helper=True
)
z_non_closure_parametrized_helper, z_non_closure_binned_helper = (
    muon_calibration.make_Z_non_closure_helpers(
        args, calib_filepaths, closure_filepaths
    )
)

mc_calibration_helper, data_calibration_helper, calibration_uncertainty_helper = (
    muon_calibration.make_muon_calibration_helpers(args, era=era)
)

closure_unc_helper = muon_calibration.make_closure_uncertainty_helper(
    common.closure_filepaths["parametrized"]
)
closure_unc_helper_A = muon_calibration.make_uniform_closure_uncertainty_helper(
    0, common.correlated_variation_base_size["A"]
)
closure_unc_helper_M = muon_calibration.make_uniform_closure_uncertainty_helper(
    2, common.correlated_variation_base_size["M"]
)

smearing_helper, smearing_uncertainty_helper = (
    (None, None) if args.noSmearing else muon_calibration.make_muon_smearing_helpers()
)

smearinggradhelper = muon_calibration.make_smearing_grad_helper()

bias_helper = muon_calibration.make_muon_bias_helpers(args)

(
    pixel_multiplicity_helper,
    pixel_multiplicity_uncertainty_helper,
    pixel_multiplicity_uncertainty_helper_stat,
) = muon_calibration.make_pixel_multiplicity_helpers(
    reverse_variations=args.reweightPixelMultiplicity
)

if args.nToysMC > 0:
    seed_data = 2 * args.randomSeedForToys
    seed_mc = 2 * args.randomSeedForToys + 1
    toy_helper_data = ROOT.wrem.ToyHelper(
        args.nToysMC, seed_data, 1, ROOT.ROOT.GetThreadPoolSize()
    )
    toy_helper_mc = ROOT.wrem.ToyHelper(
        args.nToysMC,
        seed_mc,
        args.varianceScalingForToys,
        ROOT.ROOT.GetThreadPoolSize(),
    )
    axis_toys = hist.axis.Integer(
        0, args.nToysMC, underflow=False, overflow=False, name="toys"
    )
if args.splitSampleInN > 1:
    seed_mc_split = 2 * args.randomSeedForSplit + 2
    rand_helper_mc = ROOT.wrem.RandomUniformHelper(
        args.splitSampleInN, seed_mc_split, ROOT.ROOT.GetThreadPoolSize()
    )
    axis_split = hist.axis.Integer(
        0,
        args.splitSampleInN,
        underflow=False,
        overflow=False,
        name="sample_split",
    )
if args.jackknifeN > 0:
    seed_mc_jackknife = 2 * args.randomSeedForJackknife + 1
    jackknife_helper = ROOT.wrem.JackknifeHelper(
        args.jackknifeN,
        args.jackknifeEfficiency,
        seed_mc_jackknife,
        ROOT.ROOT.GetThreadPoolSize(),
    )
    axis_jackknife = hist.axis.Integer(
        0, args.jackknifeN, underflow=False, overflow=False, name="jackknife_sample"
    )

theory_corrs = [*args.theoryCorr, *args.ewTheoryCorr]
corr_helpers = theory_corrections.load_corr_helpers(
    [d.name for d in datasets if d.name in common.vprocs], theory_corrs
)

def write_event_csv(df, dataset, cols):
    if not args.saveEventCsv:
        return

    outdir = args.eventCsvOutDir or args.outfolder or os.getcwd()
    os.makedirs(outdir, exist_ok=True)

    available = {str(c) for c in df.GetColumnNames()}
    missing = [c for c in cols if c not in available]
    if missing:
        logger.warning(f"Missing columns for CSV export: {missing}")
    cols = [c for c in cols if c in available]
    if not cols:
        logger.warning("No valid columns found for CSV export; skipping")
        return

    df_csv = df
    if args.eventCsvMaxEvents and args.eventCsvMaxEvents > 0:
        df_csv = df_csv.Range(args.eventCsvMaxEvents)

    data = df_csv.AsNumpy(cols)
    nrows = len(next(iter(data.values()))) if data else 0

    safe_name = dataset.name.replace("/", "_")
    filename = f"{args.eventCsvPrefix}_{safe_name}.csv"
    outpath = os.path.join(outdir, filename)

    with open(outpath, "w", newline="") as handle:
        writer = csv.writer(handle)
        writer.writerow(cols)
        for row in zip(*(data[c] for c in cols)):
            writer.writerow(row)

    logger.info(f"Wrote {nrows} events to {outpath}")


def build_graph(df, dataset):
    logger.info(f"build graph for dataset: {dataset.name}")
    results = []
    isW = dataset.name in common.wprocs
    isZ = dataset.name in common.zprocs
    isWorZ = isW or isZ

    theory_helpers = {}
    if isWorZ:
        theory_helpers = theory_helpers_procs[dataset.name[0]]

    cvh_helper = data_calibration_helper if dataset.is_data else mc_calibration_helper
    jpsi_helper = data_jpsi_crctn_helper if dataset.is_data else mc_jpsi_crctn_helper
    if dataset.is_data:
        df = df.DefinePerSample("weight", "1.0")
    else:
        df = df.Define("weight", "std::copysign(1.0, genWeight)")

    df = df.DefinePerSample("unity", "1.0")
    df = df.Define(
        "isEvenEvent", f"event % 2 {'!=' if args.flipEventNumberSplitting else '=='} 0"
    )

    if args.nToysMC > 0:
        if dataset.is_data:
            df = df.Define("toyIdxs", toy_helper_data, ["rdfslot_"])
        else:
            df = df.Define("toyIdxs", toy_helper_mc, ["rdfslot_"])

    if args.splitSampleInN > 1 and not dataset.is_data:
        df = df.Define("sample_n", rand_helper_mc, ["rdfslot_"])

    if args.jackknifeN > 0 and not dataset.is_data:
        df = df.Define("jackknife_sample", jackknife_helper, ["rdfslot_"])

    weightsum = df.SumAndCount("weight")

    axes = nominal_axes
    cols = nominal_cols

    if args.addRunAxis and dataset.is_data:
        run_edges = common.run_edges
        axes = [
            *axes,
            hist.axis.Variable(
                run_edges + 0.5, name="run", underflow=False, overflow=False
            ),
        ]
        cols = [*cols, "run"]

    if args.unfolding and dataset.group == "Zmumu":
        df = unfolder_z.add_gen_histograms(
            args, df, results, dataset, corr_helpers, theory_helpers=theory_helpers
        )

        if not unfolder_z.poi_as_noi:
            axes = [
                *nominal_axes,
                *unfolder_z.unfolding_axes[unfolder_z.unfolding_levels[-1]],
            ]
            cols = [
                *nominal_cols,
                *unfolder_z.unfolding_cols[unfolder_z.unfolding_levels[-1]],
            ]

    if args.xnormOnly:
        return results, weightsum

    if not args.noAuxiliaryHistograms and isZ and len(auxiliary_gen_axes):
        # gen level variables before selection
        df_gen = df
        df_gen = df_gen.DefinePerSample("exp_weight", "1.0")
        df_gen = theory_tools.define_theory_weights_and_corrs(
            df_gen, dataset.name, corr_helpers, args, theory_helpers=theory_helpers
        )

        for obs in auxiliary_gen_axes:
            results.append(
                df_gen.HistoBoost(
                    f"gen_{obs}", [all_axes[obs]], [obs, "nominal_weight"]
                )
            )
            syst_tools.add_theory_hists(
                results,
                df_gen,
                args,
                dataset.name,
                corr_helpers,
                theory_helpers,
                [all_axes[obs]],
                [obs],
                base_name=f"gen_{obs}",
                for_wmass=False,
            )

    df = df.Filter(muon_selections.hlt_string(era))

    df = muon_selections.veto_electrons(df)
    df = muon_selections.apply_met_filters(df)

    df = muon_calibration.define_corrected_muons(
        df, cvh_helper, jpsi_helper, args, dataset, smearing_helper, bias_helper
    )

    df = muon_selections.select_veto_muons(df, nMuons=2)
    isoThreshold = args.isolationThreshold
    passIsoBoth = args.muonIsolation[0] + args.muonIsolation[1] == 2
    df = muon_selections.select_good_muons(
        df,
        args.pt[1],
        args.pt[2],
        dataset.group,
        nMuons=2,
        use_trackerMuons=args.trackerMuons,
        use_isolation=passIsoBoth,
        isoBranch=isoBranch,
        isoThreshold=isoThreshold,
        requirePixelHits=args.requirePixelHits,
    )

    df = muon_selections.define_trigger_muons(
        df, dilepton=args.useDileptonTriggerSelection
    )

    # iso cut applied here, if requested, because it needs the definition of trigMuons and nonTrigMuons from muon_selections.define_trigger_muons
    if not passIsoBoth:
        df = muon_selections.apply_iso_muons(
            df, args.muonIsolation[0], args.muonIsolation[1], isoBranch, isoThreshold
        )

    df = df.Define("trigMuons_passIso0", f"{isoBranch}[trigMuons][0] < {isoThreshold}")
    df = df.Define(
        "nonTrigMuons_passIso0", f"{isoBranch}[nonTrigMuons][0] < {isoThreshold}"
    )

    df = muon_selections.select_z_candidate(df, mass_min, mass_max, ptll_min, ptll_max)

    df = muon_selections.select_standalone_muons(
        df, dataset, args.trackerMuons, "trigMuons"
    )
    df = muon_selections.select_standalone_muons(
        df, dataset, args.trackerMuons, "nonTrigMuons"
    )

    if args.useDileptonTriggerSelection:
        df = muon_selections.apply_triggermatching_muon(
            df, dataset, "trigMuons", "nonTrigMuons", era=era
        )
        df = df.Alias("muonsMinus_pt0", "trigMuons_pt0")
        df = df.Alias("muonsPlus_pt0", "nonTrigMuons_pt0")
        df = df.Alias("muonsMinus_eta0", "trigMuons_eta0")
        df = df.Alias("muonsPlus_eta0", "nonTrigMuons_eta0")
        df = df.Alias("muonsMinus_mom4", "trigMuons_mom4")
        df = df.Alias("muonsPlus_mom4", "nonTrigMuons_mom4")
    else:
        df = muon_selections.apply_triggermatching_muon(
            df, dataset, "trigMuons", era=era
        )
        df = df.Define("trigMuon_isNegative", "trigMuons_charge0 == -1")
        df = df.Define(
            "muonsMinus_pt0", "trigMuon_isNegative ? trigMuons_pt0 : nonTrigMuons_pt0"
        )
        df = df.Define(
            "muonsPlus_pt0", "trigMuon_isNegative ? nonTrigMuons_pt0 : trigMuons_pt0"
        )
        df = df.Define(
            "muonsMinus_eta0",
            "trigMuon_isNegative ? trigMuons_eta0 : nonTrigMuons_eta0",
        )
        df = df.Define(
            "muonsPlus_eta0", "trigMuon_isNegative ? nonTrigMuons_eta0 : trigMuons_eta0"
        )
        df = df.Define(
            "muonsMinus_mom4",
            "trigMuon_isNegative ? trigMuons_mom4 : nonTrigMuons_mom4",
        )
        df = df.Define(
            "muonsPlus_mom4", "trigMuon_isNegative ? nonTrigMuons_mom4 : trigMuons_mom4"
        )

    useTnpMuonVarForSF = args.useTnpMuonVarForSF
    # in principle these are only needed for MC,
    # but one may want to compare tnp and corrected variables also for data
    if useTnpMuonVarForSF:
        df = df.Define("trigMuons_tnpPt0", "Muon_pt[trigMuons][0]")
        df = df.Define("trigMuons_tnpEta0", "Muon_eta[trigMuons][0]")
        df = df.Define("trigMuons_tnpCharge0", "Muon_charge[trigMuons][0]")
        df = df.Define("nonTrigMuons_tnpPt0", "Muon_pt[nonTrigMuons][0]")
        df = df.Define("nonTrigMuons_tnpEta0", "Muon_eta[nonTrigMuons][0]")
        df = df.Define("nonTrigMuons_tnpCharge0", "Muon_charge[nonTrigMuons][0]")
    else:
        df = df.Alias("trigMuons_tnpPt0", "trigMuons_pt0")
        df = df.Alias("trigMuons_tnpEta0", "trigMuons_eta0")
        df = df.Alias("trigMuons_tnpCharge0", "trigMuons_charge0")
        df = df.Alias("nonTrigMuons_tnpPt0", "nonTrigMuons_pt0")
        df = df.Alias("nonTrigMuons_tnpEta0", "nonTrigMuons_eta0")
        df = df.Alias("nonTrigMuons_tnpCharge0", "nonTrigMuons_charge0")
        #

    # df = df.Define("ptll", "ll_mom4.pt()")
    df = df.Define("yll", "ll_mom4.Rapidity()")
    df = df.Define("absYll", "std::fabs(yll)")
    df = df.Define("xmaxll", "exp(yll)*mll/std::sqrt(13000*13000)")
    df = df.Define("xminll", "exp(-yll)*mll/std::sqrt(13000*13000)")
    # df = df.Define("xminmax_ll", "std::vector<double>{xminll, xmaxll}")
    # "renaming" to write out corresponding axis
    df = df.Alias("ptMinus", "muonsMinus_pt0")
    df = df.Alias("ptPlus", "muonsPlus_pt0")
    df = df.Alias("etaMinus", "muonsMinus_eta0")
    df = df.Alias("etaPlus", "muonsPlus_eta0")
    df = df.Define("absEtaMinus", "std::fabs(etaMinus)")
    df = df.Define("absEtaPlus", "std::fabs(etaPlus)")
    df = df.Define("etaAbsEta", "absEtaMinus > absEtaPlus ? etaMinus : etaPlus")

    df = df.Define(
        "etaRegionRange",
        "(std::abs(muonsPlus_eta0) > 0.9) + (std::abs(muonsMinus_eta0) > 0.9)",
    )  # eta region: 0: barrel-barrel, 1: endcap-barrel, 2: endcap-endcap
    df = df.Define(
        "etaRegionSign", "(muonsPlus_eta0 > 0) + (muonsMinus_eta0 > 0)"
    )  # eta region: 0: both muons in negative eta, 1: one muon in negative eta, 2: both muons in positive eta

    df = df.Define("etaSum", "muonsPlus_eta0 + muonsMinus_eta0")
    df = df.Define("etaDiff", "muonsPlus_eta0 - muonsMinus_eta0")  # plus - minus

    df = df.Define(
        "csSineCosThetaPhill",
        "wrem::csSineCosThetaPhi(muonsPlus_mom4, muonsMinus_mom4)",
    )
    df = df.Define("cosThetaStarll", "csSineCosThetaPhill.costheta")
    df = df.Define("phiStarll", "csSineCosThetaPhill.phi()")

    if args.csVarsHist:
        for c, h, a in (
            (
                "phiStarll_quantile",
                quantile_helper_csVars[0],
                ["phiStarll", "ptll", "absYll"],
            ),
            (
                "cosThetaStarll_quantile",
                quantile_helper_csVars[1],
                ["cosThetaStarll", "phiStarll", "ptll", "absYll"],
            ),
        ):
            if [a for a in h.axes.name] != a:
                raise RuntimeError(
                    f"Invalid helper axes: {[a for a in h.axes.name]} != {a}"
                )

            df = df.Define(c, h, a)

    # TODO might need to add an explicit cut on trigMuons_pt0 in case nominal pt range
    # extends below 26 GeV e.g. for calibration test purposes
    df = df.Define("trigMuons_abseta0", "std::fabs(trigMuons_eta0)")

    if dataset.is_data:
        df = df.DefinePerSample("nominal_weight", "1.0")
        cvhName = "cvh"
        write_event_csv(df, dataset, args.eventCsvCols)
    else:
        cvhName = "cvhideal"

    axis_eta = hist.axis.Regular(int(args.eta[0]), args.eta[1], args.eta[2], name="eta")
    axis_pt = hist.axis.Regular(int(args.pt[0]), args.pt[1], args.pt[2], name="pt")
    axis_charge = common.axis_charge
    axis_nvalidpixel = hist.axis.Integer(0, 10, name="nvalidpixel")

    df = df.Define(
        f"trigMuons_{cvhName}NValidPixelHits0",
        f"Muon_{cvhName}NValidPixelHits[trigMuons][0]",
    )
    df = df.Define(
        f"nonTrigMuons_{cvhName}NValidPixelHits0",
        f"Muon_{cvhName}NValidPixelHits[nonTrigMuons][0]",
    )

    logger.debug(f"Define weights and store nominal histograms")

    if dataset.is_data:
        if args.nToysMC > 0:
            axes = [*axes, axis_toys]
            cols = [*cols, "toyIdxs"]
        results.append(df.HistoBoost("nominal", axes, cols))
    else:
        df = df.Define("weight_pu", pileup_helper, ["Pileup_nTrueInt"])
        df = df.Define("weight_vtx", vertex_helper, ["GenVtx_z", "Pileup_nTrueInt"])

        if era == "2016PostVFP":
            df = df.Define(
                "weight_newMuonPrefiringSF",
                muon_prefiring_helper,
                [
                    "Muon_correctedEta",
                    "Muon_correctedPt",
                    "Muon_correctedPhi",
                    "Muon_correctedCharge",
                    "Muon_looseId",
                ],
            )
            weight_expr = (
                "weight_pu*weight_newMuonPrefiringSF*L1PreFiringWeight_ECAL_Nom"
            )
        else:
            weight_expr = (
                "weight_pu*L1PreFiringWeight_Muon_Nom*L1PreFiringWeight_ECAL_Nom"
            )

        if not args.noVertexWeight:
            weight_expr += "*weight_vtx"

        muonVarsForSF = [
            "tnpPt0",
            "tnpEta0",
            "SApt0",
            "SAeta0",
            "tnpUT0",
            "tnpCharge0",
            "passIso0",
        ]
        if args.useDileptonTriggerSelection:
            muonVarsForSF.append("passTrigger0")
        # careful, first all trig variables, then all nonTrig
        columnsForSF = [
            f"{t}Muons_{v}" for t in ["trig", "nonTrig"] for v in muonVarsForSF
        ]

        df = muon_selections.define_muon_uT_variable(
            df,
            isWorZ,
            smooth3dsf=args.smooth3dsf,
            colNamePrefix="trigMuons",
            addWithTnpMuonVar=useTnpMuonVarForSF,
        )
        df = muon_selections.define_muon_uT_variable(
            df,
            isWorZ,
            smooth3dsf=args.smooth3dsf,
            colNamePrefix="nonTrigMuons",
            addWithTnpMuonVar=useTnpMuonVarForSF,
        )
        # ut is defined in muon_selections.define_muon_uT_variable
        if not useTnpMuonVarForSF:
            df = df.Alias("trigMuons_tnpUT0", "trigMuons_uT0")
            df = df.Alias("nonTrigMuons_tnpUT0", "nonTrigMuons_uT0")

        if not args.smooth3dsf:
            columnsForSF.remove("trigMuons_tnpUT0")
            columnsForSF.remove("nonTrigMuons_tnpUT0")

        if not args.noScaleFactors:
            # FIXME: add flags for pass_trigger for both leptons
            df = df.Define(
                "weight_fullMuonSF_withTrackingReco",
                muon_efficiency_helper,
                columnsForSF,
            )
            weight_expr += "*weight_fullMuonSF_withTrackingReco"

        # prepare inputs for pixel multiplicity helpers
        df = df.DefinePerSample(
            "MuonNonTrigTrig_triggerCat",
            "ROOT::VecOps::RVec<wrem::TriggerCat>{wrem::TriggerCat::nonTriggering, wrem::TriggerCat::triggering}",
        )
        df = df.Define(
            "MuonNonTrigTrig_eta",
            "ROOT::VecOps::RVec<float>{nonTrigMuons_eta0, trigMuons_eta0}",
        )
        df = df.Define(
            "MuonNonTrigTrig_pt",
            "ROOT::VecOps::RVec<float>{nonTrigMuons_pt0, trigMuons_pt0}",
        )
        df = df.Define(
            "MuonNonTrigTrig_charge",
            "ROOT::VecOps::RVec<int>{nonTrigMuons_charge0, trigMuons_charge0}",
        )
        df = df.Define(
            f"MuonNonTrigTrig_{cvhName}NValidPixelHits",
            f"ROOT::VecOps::RVec<int>{{nonTrigMuons_{cvhName}NValidPixelHits0, trigMuons_{cvhName}NValidPixelHits0}}",
        )

        pixel_multiplicity_cols = [
            "MuonNonTrigTrig_triggerCat",
            "MuonNonTrigTrig_eta",
            "MuonNonTrigTrig_pt",
            "MuonNonTrigTrig_charge",
            f"MuonNonTrigTrig_{cvhName}NValidPixelHits",
        ]

        if args.reweightPixelMultiplicity:
            df = df.Define(
                "weight_pixel_multiplicity",
                pixel_multiplicity_helper,
                pixel_multiplicity_cols,
            )
            weight_expr += "*weight_pixel_multiplicity"

        logger.debug(f"Experimental weight defined: {weight_expr}")
        df = df.Define("exp_weight", weight_expr)
        df = theory_tools.define_theory_weights_and_corrs(
            df, dataset.name, corr_helpers, args, theory_helpers=theory_helpers
        )

        write_event_csv(df, dataset, args.eventCsvCols)

        results.append(
            df.HistoBoost(
                "weight",
                [hist.axis.Regular(100, -2, 2)],
                ["nominal_weight"],
                storage=hist.storage.Double(),
            )
        )

        if args.nToysMC > 0 or args.splitSampleInN > 1 or args.jackknifeN > 1:
            results.append(
                df.HistoBoost("nominal_asimov", axes, [*cols, "nominal_weight"])
            )
        if args.nToysMC > 0:
            axes = [*axes, axis_toys]
            cols = [*cols, "toyIdxs"]
        if args.splitSampleInN > 1:
            axes = [*axes, axis_split]
            cols = [*cols, "sample_n"]
        if args.jackknifeN > 1:
            axes = [*axes, axis_jackknife]
            cols = [*cols, "jackknife_sample"]

        results.append(df.HistoBoost("nominal", axes, [*cols, "nominal_weight"]))

        if isZ:
            # theory agnostic stuff
            theoryAgnostic_axes, theoryAgnostic_cols = (
                differential.get_theoryAgnostic_axes(
                    ptV_bins=[],
                    absYV_bins=[],
                    ptV_flow=True,
                    absYV_flow=True,
                    wlike=True,
                )
            )
            axis_helicity = helicity_utils.axis_helicity_multidim

            df_theory_agnostic = theoryAgnostic_tools.define_helicity_weights(
                df, is_z=True
            )
            noiAsPoiHistName = Datagroups.histName(
                "nominal", syst="yieldsTheoryAgnostic"
            )
            logger.debug(
                f"Creating special histogram '{noiAsPoiHistName}' for theory agnostic to treat POIs as NOIs"
            )
            results.append(
                df_theory_agnostic.HistoBoost(
                    noiAsPoiHistName,
                    [*axes, *theoryAgnostic_axes],
                    [*cols, *theoryAgnostic_cols, "nominal_weight_helicity"],
                    tensor_axes=[axis_helicity],
                )
            )

    # histograms for corrections/uncertainties for pixel hit multiplicity

    # hNValidPixelHitsTrig = df.HistoBoost("hNValidPixelHitsTrig", [axis_eta, axis_nvalidpixel], ["trigMuons_eta0", f"trigMuons_{cvhName}NValidPixelHits0", "nominal_weight"])
    # results.append(hNValidPixelHitsTrig)
    #
    # hNValidPixelHitsNonTrig = df.HistoBoost("hNValidPixelHitsNonTrig", [axis_eta, axis_nvalidpixel], ["nonTrigMuons_eta0", f"nonTrigMuons_{cvhName}NValidPixelHits0", "nominal_weight"])
    # results.append(hNValidPixelHitsNonTrig)

    hNValidPixelHitsTrig = df.HistoBoost(
        "hNValidPixelHitsTrig",
        [axis_eta, axis_pt, axis_charge, axis_nvalidpixel],
        [
            "trigMuons_eta0",
            "trigMuons_pt0",
            "trigMuons_charge0",
            f"trigMuons_{cvhName}NValidPixelHits0",
            "nominal_weight",
        ],
    )
    results.append(hNValidPixelHitsTrig)

    hNValidPixelHitsNonTrig = df.HistoBoost(
        "hNValidPixelHitsNonTrig",
        [axis_eta, axis_pt, axis_charge, axis_nvalidpixel],
        [
            "nonTrigMuons_eta0",
            "nonTrigMuons_pt0",
            "nonTrigMuons_charge0",
            f"nonTrigMuons_{cvhName}NValidPixelHits0",
            "nominal_weight",
        ],
    )
    results.append(hNValidPixelHitsNonTrig)

    if args.unfolding and args.poiAsNoi and dataset.group == "Zmumu":
        unfolder_z.add_poi_as_noi_histograms(
            df,
            results,
            nominal_axes,
            nominal_cols,
        )

    if args.makeCSQuantileHists:
        results.append(
            df.HistoBoost(
                f"nominal_csQuantiles",
                [
                    all_axes[o]
                    for o in ["ptll", "absYll", "phiStarll", "cosThetaStarll"]
                ],
                ["ptll", "absYll", "phiStarll", "cosThetaStarll"],
            )
        )

    if not args.noAuxiliaryHistograms:
        for obs in [
            ["ptll", "yll"],
            "mll",
            "yll",
            "xmaxll",
            "xminll",
            # "xminmax_ll",
            "cosThetaStarll",
            "phiStarll",
            "etaPlus",
            "etaMinus",
            "ptPlus",
            "ptMinus",
        ]:
            if isinstance(obs, str):
                obs = [obs]
            obs_name = f"nominal_{'_'.join(obs)}"
            obs_axes = [all_axes[o] for o in obs]

            if dataset.is_data:
                results.append(df.HistoBoost(obs_name, obs_axes, obs))
            else:
                results.append(
                    df.HistoBoost(obs_name, obs_axes, [*obs, "nominal_weight"])
                )
                if isWorZ and not args.onlyMainHistograms:
                    df = syst_tools.add_theory_hists(
                        results,
                        df,
                        args,
                        dataset.name,
                        corr_helpers,
                        theory_helpers,
                        obs_axes,
                        obs,
                        base_name=obs_name,
                        for_wmass=False,
                    )

    if not args.noAuxiliaryHistograms and isZ:
        # gen level variables
        for obs in auxiliary_gen_axes:
            results.append(
                df.HistoBoost(
                    f"nominal_{obs}", [all_axes[obs]], [obs, "nominal_weight"]
                )
            )
            if not args.onlyMainHistograms:
                df = syst_tools.add_theory_hists(
                    results,
                    df,
                    args,
                    dataset.name,
                    corr_helpers,
                    theory_helpers,
                    [all_axes[obs]],
                    [obs],
                    base_name=f"nominal_{obs}",
                    for_wmass=False,
                )

    # test plots
    if args.validationHists:
        # resolution plot
        df = df.Define("ptll_relResolution", "(ptll - postfsrPTV)/postfsrPTV")
        df = df.Define("ptll_resolution", "(ptll - postfsrPTV)")
        results.append(
            df.HistoBoost(
                f"nominal_relResolution",
                [all_axes["ptll_resolution"], all_axes["ptll"], axis_absYll],
                ["ptll_resolution", "postfsrPTV", "absYll", "nominal_weight"],
            )
        )
        results.append(
            df.HistoBoost(
                f"nominal_resolution",
                [all_axes["ptll_resolution"], all_axes["ptll"], axis_absYll],
                ["ptll_resolution", "postfsrPTV", "absYll", "nominal_weight"],
            )
        )

    if args.validationHists and args.useDileptonTriggerSelection:
        df_plusTrig = df.Filter("trigMuons_passTrigger0")
        df_minusTrig = df.Filter("nonTrigMuons_passTrigger0")
        df_bothTrig = df.Filter("trigMuons_passTrigger0 && nonTrigMuons_passTrigger0")
        df_plusTrigOnly = df.Filter(
            "trigMuons_passTrigger0 && !nonTrigMuons_passTrigger0"
        )
        df_minusTrigOnly = df.Filter(
            "nonTrigMuons_passTrigger0 && !trigMuons_passTrigger0"
        )
        for obs in ["etaPlus", "etaMinus", "ptPlus", "ptMinus"]:
            if dataset.is_data:
                results.append(
                    df_plusTrig.HistoBoost(
                        f"nominal_{obs}_plusTrig", [all_axes[obs]], [obs]
                    )
                )
                results.append(
                    df_minusTrig.HistoBoost(
                        f"nominal_{obs}_minusTrig", [all_axes[obs]], [obs]
                    )
                )
                results.append(
                    df_bothTrig.HistoBoost(
                        f"nominal_{obs}_bothTrig", [all_axes[obs]], [obs]
                    )
                )
                results.append(
                    df_plusTrigOnly.HistoBoost(
                        f"nominal_{obs}_plusTrigOnly", [all_axes[obs]], [obs]
                    )
                )
                results.append(
                    df_minusTrigOnly.HistoBoost(
                        f"nominal_{obs}_minusTrigOnly", [all_axes[obs]], [obs]
                    )
                )
            else:
                results.append(
                    df_plusTrig.HistoBoost(
                        f"nominal_{obs}_plusTrig",
                        [all_axes[obs]],
                        [obs, "nominal_weight"],
                    )
                )
                results.append(
                    df_minusTrig.HistoBoost(
                        f"nominal_{obs}_minusTrig",
                        [all_axes[obs]],
                        [obs, "nominal_weight"],
                    )
                )
                results.append(
                    df_bothTrig.HistoBoost(
                        f"nominal_{obs}_bothTrig",
                        [all_axes[obs]],
                        [obs, "nominal_weight"],
                    )
                )
                results.append(
                    df_plusTrigOnly.HistoBoost(
                        f"nominal_{obs}_plusTrigOnly",
                        [all_axes[obs]],
                        [obs, "nominal_weight"],
                    )
                )
                results.append(
                    df_minusTrigOnly.HistoBoost(
                        f"nominal_{obs}_minusTrigOnly",
                        [all_axes[obs]],
                        [obs, "nominal_weight"],
                    )
                )

    if not dataset.is_data:

        df = df.Define(
            "Mupluscor_mom4",
            "trigMuons_charge0 == 1 ? trigMuons_mom4 : nonTrigMuons_mom4",
        )
        df = df.Define(
            "Muminuscor_mom4",
            "trigMuons_charge0 == -1 ? trigMuons_mom4 : nonTrigMuons_mom4",
        )

        df = df.Define(
            "parmgrads_k7",
            "wrem::parmgrads_k7_t(Mupluscor_mom4, Muminuscor_mom4, nominal_weight)",
        )
        df = df.Define(
            "parmgradsres_k3",
            smearinggradhelper,
            ["Mupluscor_mom4", "Muminuscor_mom4", "nominal_weight"],
        )

        df = df.Define(
            "etav", "std::array<double, 2>{Mupluscor_mom4.eta(), Muminuscor_mom4.eta()}"
        )
        df = df.Define(
            "phiv", "std::array<double, 2>{Mupluscor_mom4.phi(), Muminuscor_mom4.phi()}"
        )
        df = df.Define(
            "ptv", "std::array<double, 2>{Mupluscor_mom4.pt(), Muminuscor_mom4.pt()}"
        )

        axis_corr_eta = hist.axis.Regular(48, -2.4, 2.4, name="corr_eta")
        axis_corr_phi = hist.axis.Regular(
            1, -np.pi, np.pi, circular=True, name="corr_phi"
        )
        axis_corr_parms = hist.axis.StrCategory(
            ["A_k", "e_k", "M_k", "M_lambda", "A_phi", "e_phi", "M_phi"],
            name="corr_parms",
        )
        axis_res_parms = hist.axis.StrCategory(["a", "c", "b"], name="res_parms")

        parmgrad_axes = [*axes[:-1], axis_corr_eta, axis_corr_phi]
        parmgrad_cols = [*cols[:-1], "etav", "phiv", "parmgrads_k7"]

        hparmgrads = df.HistoBoost(
            "hparmgrads", parmgrad_axes, parmgrad_cols, tensor_axes=[axis_corr_parms]
        )
        results.append(hparmgrads)

        parmgradres_axes = parmgrad_axes
        parmgradres_cols = [*cols[:-1], "etav", "phiv", "parmgradsres_k3"]

        hparmgradsres = df.HistoBoost(
            "hparmgradsres",
            parmgradres_axes,
            parmgradres_cols,
            tensor_axes=[axis_res_parms],
        )
        results.append(hparmgradsres)

    if not dataset.is_data and not args.onlyMainHistograms:

        df = syst_tools.add_muon_efficiency_unc_hists(
            results,
            df,
            muon_efficiency_helper_stat,
            muon_efficiency_helper_syst,
            axes,
            cols,
            what_analysis=thisAnalysis,
            smooth3D=args.smooth3dsf,
        )
        for es in common.muonEfficiency_altBkgSyst_effSteps:
            df = syst_tools.add_muon_efficiency_unc_hists_altBkg(
                results,
                df,
                muon_efficiency_helper_syst_altBkg[es],
                axes,
                cols,
                what_analysis=thisAnalysis,
                step=es,
            )

        df = syst_tools.add_L1Prefire_unc_hists(
            results,
            df,
            axes,
            cols,
            helper_stat=muon_prefiring_helper_stat,
            helper_syst=muon_prefiring_helper_syst,
        )

        if isWorZ:

            df = syst_tools.add_theory_hists(
                results,
                df,
                args,
                dataset.name,
                corr_helpers,
                theory_helpers,
                axes,
                cols,
                for_wmass=False,
            )

            reco_sel = "vetoMuonsPre"
            require_prompt = "tau" not in dataset.name
            df = muon_calibration.define_genFiltered_recoMuonSel(
                df, reco_sel, require_prompt
            )
            reco_sel_GF = muon_calibration.getColName_genFiltered_recoMuonSel(
                reco_sel, require_prompt
            )
            df = muon_calibration.define_matched_gen_muons_kinematics(df, reco_sel_GF)
            df = muon_calibration.calculate_matched_gen_muon_kinematics(df, reco_sel_GF)
            df = muon_calibration.define_matched_reco_muon_kinematics(df, reco_sel_GF)

            ####################################################
            # nuisances from the muon momemtum scale calibration
            if args.muonCorrData in ["massfit", "lbl_massfit"]:
                input_kinematics = [
                    f"{reco_sel_GF}_recoPt",
                    f"{reco_sel_GF}_recoEta",
                    f"{reco_sel_GF}_recoCharge",
                    f"{reco_sel_GF}_genPt",
                    f"{reco_sel_GF}_genEta",
                    f"{reco_sel_GF}_genCharge",
                ]
                if diff_weights_helper:
                    df = df.Define(
                        f"{reco_sel_GF}_response_weight",
                        diff_weights_helper,
                        [*input_kinematics],
                    )
                    input_kinematics.append(f"{reco_sel_GF}_response_weight")

                # muon scale variation from stats. uncertainty on the jpsi massfit
                df = df.Define(
                    "nominal_muonScaleSyst_responseWeights_tensor",
                    data_jpsi_crctn_unc_helper,
                    [*input_kinematics, "nominal_weight"],
                )
                muonScaleSyst_responseWeights = df.HistoBoost(
                    "nominal_muonScaleSyst_responseWeights",
                    axes,
                    [*cols, "nominal_muonScaleSyst_responseWeights_tensor"],
                    tensor_axes=data_jpsi_crctn_unc_helper.tensor_axes,
                    storage=hist.storage.Double(),
                )
                results.append(muonScaleSyst_responseWeights)

                df = muon_calibration.add_resolution_uncertainty(
                    df, axes, results, cols, smearing_uncertainty_helper, reco_sel_GF
                )

                # add pixel multiplicity uncertainties
                df = df.Define(
                    "nominal_pixelMultiplicitySyst_tensor",
                    pixel_multiplicity_uncertainty_helper,
                    [*pixel_multiplicity_cols, "nominal_weight"],
                )
                hist_pixelMultiplicitySyst = df.HistoBoost(
                    "nominal_pixelMultiplicitySyst",
                    axes,
                    [*cols, "nominal_pixelMultiplicitySyst_tensor"],
                    tensor_axes=pixel_multiplicity_uncertainty_helper.tensor_axes,
                    storage=hist.storage.Double(),
                )
                results.append(hist_pixelMultiplicitySyst)

                if args.pixelMultiplicityStat:
                    df = df.Define(
                        "nominal_pixelMultiplicityStat_tensor",
                        pixel_multiplicity_uncertainty_helper_stat,
                        [*pixel_multiplicity_cols, "nominal_weight"],
                    )
                    hist_pixelMultiplicityStat = df.HistoBoost(
                        "nominal_pixelMultiplicityStat",
                        axes,
                        [*cols, "nominal_pixelMultiplicityStat_tensor"],
                        tensor_axes=pixel_multiplicity_uncertainty_helper_stat.tensor_axes,
                        storage=hist.storage.Double(),
                    )
                    results.append(hist_pixelMultiplicityStat)

                if args.nonClosureScheme in ["A-M-separated", "A-only"]:
                    # add the ad-hoc Z non-closure nuisances from the jpsi massfit to muon scale unc
                    df = df.DefinePerSample("AFlag", "0x01")
                    df = df.Define(
                        "Z_non_closure_parametrized_A",
                        z_non_closure_parametrized_helper,
                        [*input_kinematics, "nominal_weight", "AFlag"],
                    )
                    hist_Z_non_closure_parametrized_A = df.HistoBoost(
                        "nominal_Z_non_closure_parametrized_A",
                        axes,
                        [*cols, "Z_non_closure_parametrized_A"],
                        tensor_axes=z_non_closure_parametrized_helper.tensor_axes,
                        storage=hist.storage.Double(),
                    )
                    results.append(hist_Z_non_closure_parametrized_A)

                if args.nonClosureScheme in [
                    "A-M-separated",
                    "binned-plus-M",
                    "M-only",
                ]:
                    df = df.DefinePerSample("MFlag", "0x04")
                    df = df.Define(
                        "Z_non_closure_parametrized_M",
                        z_non_closure_parametrized_helper,
                        [*input_kinematics, "nominal_weight", "MFlag"],
                    )
                    hist_Z_non_closure_parametrized_M = df.HistoBoost(
                        "nominal_Z_non_closure_parametrized_M",
                        axes,
                        [*cols, "Z_non_closure_parametrized_M"],
                        tensor_axes=z_non_closure_parametrized_helper.tensor_axes,
                        storage=hist.storage.Double(),
                    )
                    results.append(hist_Z_non_closure_parametrized_M)

                if args.nonClosureScheme == "A-M-combined":
                    df = df.DefinePerSample("AMFlag", "0x01 | 0x04")
                    df = df.Define(
                        "Z_non_closure_parametrized",
                        z_non_closure_parametrized_helper,
                        [*input_kinematics, "nominal_weight", "AMFlag"],
                    )
                    hist_Z_non_closure_parametrized = df.HistoBoost(
                        (
                            "Z_non_closure_parametrized_gaus"
                            if args.muonScaleVariation == "smearingWeightsGaus"
                            else "nominal_Z_non_closure_parametrized"
                        ),
                        axes,
                        [*cols, "Z_non_closure_parametrized"],
                        tensor_axes=z_non_closure_parametrized_helper.tensor_axes,
                        storage=hist.storage.Double(),
                    )
                    results.append(hist_Z_non_closure_parametrized)

                # extra uncertainties from non-closure stats
                df = df.Define(
                    "muonScaleClosSyst_responseWeights_tensor_splines",
                    closure_unc_helper,
                    [*input_kinematics, "nominal_weight"],
                )
                nominal_muonScaleClosSyst_responseWeights = df.HistoBoost(
                    "nominal_muonScaleClosSyst_responseWeights",
                    axes,
                    [*cols, "muonScaleClosSyst_responseWeights_tensor_splines"],
                    tensor_axes=closure_unc_helper.tensor_axes,
                    storage=hist.storage.Double(),
                )
                results.append(nominal_muonScaleClosSyst_responseWeights)

                # extra uncertainties for A (fully correlated)
                df = df.Define(
                    "muonScaleClosASyst_responseWeights_tensor_splines",
                    closure_unc_helper_A,
                    [*input_kinematics, "nominal_weight"],
                )
                nominal_muonScaleClosASyst_responseWeights = df.HistoBoost(
                    "nominal_muonScaleClosASyst_responseWeights",
                    axes,
                    [*cols, "muonScaleClosASyst_responseWeights_tensor_splines"],
                    tensor_axes=closure_unc_helper_A.tensor_axes,
                    storage=hist.storage.Double(),
                )
                results.append(nominal_muonScaleClosASyst_responseWeights)

                # extra uncertainties for M (fully correlated)
                df = df.Define(
                    "muonScaleClosMSyst_responseWeights_tensor_splines",
                    closure_unc_helper_M,
                    [*input_kinematics, "nominal_weight"],
                )
                nominal_muonScaleClosMSyst_responseWeights = df.HistoBoost(
                    "nominal_muonScaleClosMSyst_responseWeights",
                    axes,
                    [*cols, "muonScaleClosMSyst_responseWeights_tensor_splines"],
                    tensor_axes=closure_unc_helper_M.tensor_axes,
                    storage=hist.storage.Double(),
                )
                results.append(nominal_muonScaleClosMSyst_responseWeights)

            ####################################################

            # Don't think it makes sense to apply the mass weights to scale leptons from tau decays
            if not "tau" in dataset.name:
                syst_tools.add_muonscale_hist(
                    results,
                    df,
                    args.muonCorrEtaBins,
                    args.muonCorrMag,
                    isW,
                    axes,
                    cols,
                    muon_eta="trigMuons_eta0",
                )  ## FIXME: what muon to choose ?

    if hasattr(dataset, "out_of_acceptance"):
        # Rename dataset to not overwrite the original one
        dataset.name = dataset.name + "OOA"

    return results, weightsum


logger.debug(f"Datasets are {[d.name for d in datasets]}")
resultdict = narf.build_and_run(datasets[::-1], build_graph)

if not args.noScaleToData:
    scale_to_data(resultdict)
    aggregate_groups(datasets, resultdict, args.aggregateGroups)

write_analysis_output(
    resultdict, f"{os.path.basename(__file__).replace('py', 'hdf5')}", args
)
